{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f30d691-f246-4673-a45e-b4b7e93e7624",
   "metadata": {},
   "source": [
    "# Data 695 Project Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823703eb-4152-4879-8837-f3b39f175b8f",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "Credit card fraud remains a major concern in the financial sector, causing substantial losses for both card issuers and consumers. \n",
    "Over $34 billion is lost annually to credit card fraud, a number that is expected to rise as digital transactions continue to grow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3c7f7-8e8c-4959-aa2e-a986789b6f70",
   "metadata": {},
   "source": [
    "## Part 2 - Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd41f4fd-609b-4756-b169-2a5f9c4ef8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    mean_squared_error,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08bcbda1-70c4-45e2-813a-494d45fb9c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>merchant_category</th>\n",
       "      <th>location</th>\n",
       "      <th>device_used</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>spending_deviation_score</th>\n",
       "      <th>velocity_score</th>\n",
       "      <th>geo_anomaly_score</th>\n",
       "      <th>payment_channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-12-18</td>\n",
       "      <td>28.44</td>\n",
       "      <td>transfer</td>\n",
       "      <td>retail</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>web</td>\n",
       "      <td>False</td>\n",
       "      <td>0.23</td>\n",
       "      <td>11</td>\n",
       "      <td>0.93</td>\n",
       "      <td>wire_transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>64.88</td>\n",
       "      <td>payment</td>\n",
       "      <td>retail</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>pos</td>\n",
       "      <td>False</td>\n",
       "      <td>0.44</td>\n",
       "      <td>4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>wire_transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-26</td>\n",
       "      <td>5.68</td>\n",
       "      <td>transfer</td>\n",
       "      <td>online</td>\n",
       "      <td>Dubai</td>\n",
       "      <td>web</td>\n",
       "      <td>True</td>\n",
       "      <td>0.28</td>\n",
       "      <td>18</td>\n",
       "      <td>0.09</td>\n",
       "      <td>wire_transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>11.97</td>\n",
       "      <td>transfer</td>\n",
       "      <td>utilities</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>atm</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.63</td>\n",
       "      <td>wire_transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>191.39</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>retail</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>pos</td>\n",
       "      <td>False</td>\n",
       "      <td>1.10</td>\n",
       "      <td>12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>UPI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  amount transaction_type merchant_category   location  \\\n",
       "0  2023-12-18   28.44         transfer            retail  Singapore   \n",
       "1  2023-02-06   64.88          payment            retail    Toronto   \n",
       "2  2023-07-26    5.68         transfer            online      Dubai   \n",
       "3  2023-04-27   11.97         transfer         utilities    Toronto   \n",
       "4  2023-03-14  191.39       withdrawal            retail      Tokyo   \n",
       "\n",
       "  device_used  is_fraud  spending_deviation_score  velocity_score  \\\n",
       "0         web     False                      0.23              11   \n",
       "1         pos     False                      0.44               4   \n",
       "2         web      True                      0.28              18   \n",
       "3         atm      True                     -1.31               1   \n",
       "4         pos     False                      1.10              12   \n",
       "\n",
       "   geo_anomaly_score payment_channel  \n",
       "0               0.93   wire_transfer  \n",
       "1               0.40   wire_transfer  \n",
       "2               0.09   wire_transfer  \n",
       "3               0.63   wire_transfer  \n",
       "4               0.16             UPI  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the Cleaned Credit card transaction data\n",
    "df_cleaned = pd.read_csv('cleaned_fraud_dataset.csv')\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19299c6c-4b81-4fb9-a6dc-e8c92174925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Fraudlent Transactions: 0.33\n",
      "Proportion of Genuine Transactions: 0.67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(430927, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data in 80% train and 20% test \n",
    "\n",
    "\n",
    "# Defining the x predictor variables\n",
    "X= df_cleaned.drop(columns=('is_fraud'),axis=1)\n",
    "\n",
    "\n",
    "# Defining y, target variable\n",
    "y= df_cleaned[\"is_fraud\"]\n",
    "\n",
    "\n",
    "# Splitting into train and test data \n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y, random_state=42, test_size=0.2)\n",
    "\n",
    "\n",
    "# Calculating proportion of fraudulent and genuine in training set\n",
    "count_fraud =(y_train==1).sum() \n",
    "count_geniune =(y_train==0).sum() \n",
    "\n",
    "\n",
    "prop_fraud= count_fraud/len(y_train) \n",
    "prop_genuine= count_geniune/len(y_train) \n",
    "\n",
    "print('Proportion of Fraudlent Transactions:', round(prop_fraud,2))\n",
    "print('Proportion of Genuine Transactions:',round(prop_genuine,2))\n",
    "\n",
    "\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d4d53c-fcda-48c3-ac5f-f99d4923dcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Fraudulent Transactions: 0.3333\n",
      "Proportion of Genuine Transactions: 0.6667\n",
      "Shape of X_train: (430927, 20)\n"
     ]
    }
   ],
   "source": [
    "# Standardizing the X Features\n",
    "\n",
    "# We'll only standardize continous variabes \n",
    "\n",
    "X = df_cleaned.drop(columns=['is_fraud', 'timestamp', 'location'])  \n",
    "X = pd.get_dummies(X, drop_first=True)  \n",
    "\n",
    "# Defining target variable\n",
    "y = df_cleaned['is_fraud'].astype(int)  \n",
    "\n",
    "# Splitting into training and testing sets (80% train, 20% test), maintaining class proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Counting fraud and genuine transactions in training data\n",
    "count_fraud = (y_train == 1).sum()\n",
    "count_genuine = (y_train == 0).sum()\n",
    "\n",
    "# Computing proportions\n",
    "prop_fraud = count_fraud / len(y_train)\n",
    "prop_genuine = count_genuine / len(y_train)\n",
    "\n",
    "# Displaying results\n",
    "print('Proportion of Fraudulent Transactions:', round(prop_fraud, 4))\n",
    "print('Proportion of Genuine Transactions:', round(prop_genuine, 4))\n",
    "print(\"Shape of X_train:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90fcfc9e-be60-4628-b116-c761d6017080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row indices of continuous features: Index([345412, 480928, 366564, 214938, 100648, 500675, 218194, 230279,  40532,\n",
      "       178084,\n",
      "       ...\n",
      "       348169, 222722, 128105, 493959, 327261, 200189,  65637, 334343,  91688,\n",
      "       342457],\n",
      "      dtype='int64', length=430927)\n",
      "Continuous feature columns: ['amount', 'spending_deviation_score', 'velocity_score', 'geo_anomaly_score']\n"
     ]
    }
   ],
   "source": [
    "X_train_continous = X_train.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Print  \n",
    "print(\"Row indices of continuous features:\", X_train_continous.index)\n",
    "\n",
    "# Print\n",
    "print(\"Continuous feature columns:\", X_train_continous.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b835686-ef38-4e9a-b976-0274e88f4f26",
   "metadata": {},
   "source": [
    "\n",
    "# Logistics Regression Model\n",
    "Logistic Regression is a statistical model used for binary classification problems. It estimates the probability that a given input belongs to a particular category by applying the logistic (sigmoid) function to a linear combination of the input features. Logistic regression performs well when the relationship between the features and the target variable is approximately linear. Its strengths are its interpretability, speed, and effectiveness on linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a0cb78a-d555-49da-aacc-0e778b64e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the x predictor variables\n",
    "X= df_cleaned.drop(columns=('is_fraud'),axis=1)\n",
    "\n",
    "# Defining y, target variable\n",
    "y= df_cleaned[\"is_fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea75cc0e-60af-4a2f-8b9e-49c2c3973090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Cross-validated F1 scores: [0.40069799 0.39058916 0.40081908]\n",
      "Logistic Regression - Mean F1 score: 0.3974\n",
      "Confusion Matrix:\n",
      "[[35751 36070]\n",
      " [17836 18075]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.50      0.57     71821\n",
      "           1       0.33      0.50      0.40     35911\n",
      "\n",
      "    accuracy                           0.50    107732\n",
      "   macro avg       0.50      0.50      0.49    107732\n",
      "weighted avg       0.56      0.50      0.51    107732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Features and target variable\n",
    "X = df_cleaned.drop(columns=['is_fraud', 'timestamp', 'location'])  \n",
    "X = pd.get_dummies(X, drop_first=True)  \n",
    "y = df_cleaned['is_fraud'].astype(int)  \n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initializing and train the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear')\n",
    "\n",
    "# Cross-validation before model fitting\n",
    "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=3, scoring='f1')\n",
    "print(f\"Logistic Regression - Cross-validated F1 scores: {cv_scores}\")\n",
    "print(f\"Logistic Regression - Mean F1 score: {cv_scores.mean():.4f}\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56549ba3-dc7c-43d7-8f42-99a671a8c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3338\n",
      "Specificity: 0.4978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Ensuring the shape is valid \n",
    "if cm.shape == (2, 2):\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "    # Computing precision and specificity \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "else:\n",
    "    print(\"Confusion matrix shape is not (2,2) — check if it's a binary classification problem.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89d67b1b-af6e-4c2b-beb9-255a93ba0d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4996\n",
      "F1 Score: 0.4014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Making sure predictions exist\n",
    "if 'y_pred' in locals():\n",
    "    # Computing Accuracy and F1 Score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "else:\n",
    "    print(\"Error: 'y_pred' is not defined. Please run model prediction first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb27ac4b-63e7-4bf6-92de-8f98c6205afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.5004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculating Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2547a594-b7f0-4889-9ef2-f7848057e5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Recall: 0.5033\n",
      "Logistic Regression AUC-ROC: 0.5006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, roc_auc_score\n",
    "# Recall \n",
    "logreg_recall = recall_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Recall: {logreg_recall:.4f}\")\n",
    "\n",
    "# AUC-ROC \n",
    "logreg_auc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression AUC-ROC: {logreg_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02daadb6-df74-44b1-ad53-6a187fa7c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistics Regression Precision: 0.3338\n",
      "Logistics Regression Specificity: 0.4978\n",
      "Logistics Regression Accuracy: 0.4996\n",
      "Logistics Regression F1 Score: 0.4014\n",
      "Logistics Regression Mean Squared Error (MSE): 0.5004\n",
      "Logistic Regression Recall: 0.5033\n",
      "Logistic Regression AUC-ROC: 0.5006\n"
     ]
    }
   ],
   "source": [
    "print(f\"Logistics Regression Precision: {precision:.4f}\")\n",
    "print(f\"Logistics Regression Specificity: {specificity:.4f}\")\n",
    "print(f\"Logistics Regression Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Logistics Regression F1 Score: {f1:.4f}\")\n",
    "print(f\"Logistics Regression Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Logistic Regression Recall: {logreg_recall:.4f}\")\n",
    "print(f\"Logistic Regression AUC-ROC: {logreg_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e93730d-e411-42d6-9496-a2d5feb85506",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "The Logistics Regression model had an overall accuracy of **0.4996,** and an F1-score of **0.4014** on the cleaned credit-card–fraud dataset, with an MSE of **0.5004.** \n",
    "\n",
    "The model was applied to a downsampled version of our original 5-million-row dataset, consisting of approximately 500k+ transaction records. The dataset was intentionally balanced to a 2:1 ratio of genuine to fraudulent class distribution to make the model more sensitive to detecting fraud.\n",
    "\n",
    "Taken together, these metrics indicate that the model may not adequately capture the complex patterns often found in fraud scenarios and is only marginally better than random guessing. It correctly flags roughly one-third of the transactions it labels as fraudulent, and it misses about half of the genuine transactions. \n",
    "\n",
    "While it does provides a useful baseline, achieving a more reliable fraud detection will likely require advanced models such as Random Forests, CatBoost, or Neural Networks, along with techniques like SMOTE. Therefore, this outcome is just a foundational benchmark not a production-ready solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce70758-885d-4a83-a03b-06bc779454b4",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) Model\n",
    "\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm used for both classification and regression tasks. It works by finding the optimal hyperplane that maximally separates data points of different classes, thereby improving generalization. SVM is especially effective in high-dimensional spaces and is robust against overfitting, particularly when there are clear class boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "201fc027-fe7f-451a-854b-b0cbe0e0c3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Confusion Matrix:\n",
      "[[35265 36556]\n",
      " [18030 17881]]\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.49      0.56     71821\n",
      "           1       0.33      0.50      0.40     35911\n",
      "\n",
      "    accuracy                           0.49    107732\n",
      "   macro avg       0.50      0.49      0.48    107732\n",
      "weighted avg       0.55      0.49      0.51    107732\n",
      "\n",
      "Accuracy: 0.4933\n",
      "F1 Score: 0.3958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Preparing X and y\n",
    "X = df_cleaned.drop(columns=['is_fraud', 'timestamp', 'location'])  \n",
    "X = pd.get_dummies(X, drop_first=True)  \n",
    "y = df_cleaned['is_fraud'].astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train SVM with class_weight for imbalance\n",
    "svm_model = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    class_weight='balanced',  \n",
    "    random_state=42\n",
    ")\n",
    "svm_model.fit(X_train_s, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_svm = svm_model.predict(X_test_s)\n",
    "\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))\n",
    "\n",
    "print(\"\\nSVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_svm):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644d570-a4e6-4785-aeb1-6c145b3c7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculating MSE\n",
    "mse = mean_squared_error(y_test, y_pred_svm)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09693080-4435-40ce-91e2-dc0fa3e84f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n",
    "svm_recall = recall_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Recall: {svm_recall:.4f}\")\n",
    "\n",
    "# AUC-ROC\n",
    "svm_auc = roc_auc_score(y_test, y_pred_svm)\n",
    "print(f\"SVM AUC-ROC: {svm_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f78a6d5-5636-45e5-856b-6df082c94d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"SVM Recall: {svm_recall:.4f}\")\n",
    "print(f\"SVM AUC-ROC: {svm_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d0410-b6a4-483f-8481-017c9396698e",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "The Support Vector Machine (SVM) model with an RBF kernel was applied to the cleaned and downsampled dataset containing approximately 500k+ transactions, with a class ratio of 2:1 of genuine to fraudulent. The SVM model performed very similarly to Logistic Regression, even though it’s a non-linear model, struggling with precision (high false positives), and only catching half of the fraud.\n",
    "\n",
    "Our SVM results suggest that despite SVM's theoretical strength for nonlinear classificatiois, it's not good enough for production, it does however help confirm that a simple model with an imbalanced dataset, even using non-linear kernels, can't detect fraud reliably.\n",
    "\n",
    "In conclusion, while SVM provides a useful benchmark, it is not sufficient on its own. Future efforts should prioritize more scalable and fraud-aware models such as Random Forests, or CatBoost, potentially combined with SMOTE might improve both precision and recall in fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5f5d5-3df6-4525-8031-d648e815554e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
